{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabalho final ML\n",
    "\n",
    "<a href=\"https://www.kaggle.com/code/reihanenamdari/depression-logistic-regression-and-gridsearchcv/data\">Dataset</a>\n",
    "<br>\n",
    "<hr>\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Contexto e objetivo do trabalho\n",
    "O trabalho prático da disciplina CMP263 - Aprendizagem de Máquina visa\n",
    "permitir que os alunos desenvolvam um modelo preditivo para um problema de\n",
    "interesse, praticando aspectos discutidos na disciplina relacionados ao treinamento e\n",
    "avaliação de modelos de classificação ou regressão, e interpretação dos modelos\n",
    "gerados.\n",
    "A proposta do projeto final é que os alunos aprofundem e consolidem sua\n",
    "experiência no desenvolvimento de modelos preditivos, abordando aspectos ao\n",
    "longo de toda a metodologia de treinamento de modelos, conforme discutimos em\n",
    "aula. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metodologia\n",
    "\n",
    "CRISP-DM\n",
    "\n",
    "1. Entendimento do negócio\n",
    "2. Entendimento dos dados\n",
    "3. Preparação dos dados\n",
    "4. Modelagem\n",
    "5. Validação\n",
    "6. Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Entendimento do negócio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About Dataset\n",
    "##### Context\n",
    "\n",
    "\n",
    "O dataset original foi publicado pro Frankcc no seguinte link: <a href=\"https://www.kaggle.com/datasets/diegobabativa/depression?resource=download\">Link Kaggle</a>\n",
    "\n",
    "O dataset está envolvido na análise da depressão. Os dados consistem de um estudo das condições de vida de pessoas que vivem em zonas rurais. \n",
    "\n",
    "- Conteúdo\n",
    "1. Surveyid \n",
    "2. Villeid\n",
    "3. sex\n",
    "4. Age\n",
    "5. Married\n",
    "6. Numberchildren \n",
    "7. educationlevel\n",
    "8. totalmembers (in the family) \n",
    "9. gainedasset\n",
    "10. durableasset \n",
    "11. saveasset\n",
    "12. livingexpenses \n",
    "13. otherexpenses\n",
    "14. incomingsalary \n",
    "15. incomingownfarm \n",
    "16. incomingbusiness\n",
    "17. incomingnobusiness\n",
    "18. incomingagricultural \n",
    "19. farmexpenses\n",
    "20. laborprimary \n",
    "21. lastinginvestment\n",
    "22. nolastinginvestmen\n",
    "23. depressed: [ Zero: No depressed] or [One: depressed] (Binary for target class)\n",
    "\n",
    "the main objective is to show statistic analysis and some data mining techniques.\n",
    "\n",
    "The dataset has 23 columns or dimensions and a total of 1432 rows or objects.\n",
    "\n",
    "Acknowledgements\n",
    "The original attribution is to Frankcc i\n",
    "\n",
    "Inspiration\n",
    "\n",
    "<a href=\"https://zindi.africa/competitions/busara-mental-health-prediction-challenge/data\">Busara</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problema\n",
    "\n",
    "Classificação -  objetivo é treinar o melhor classificador possível para detectar o risco de depressão, pois os dados da depressão são categóricos (0,1). Induz um classificador, gerando saídas em um domínio discreto, não ordenado \n",
    "\n",
    "yk = f (xk) ∈ {c1, c2, …, cm},\n",
    "onde m é o número de classes.\n",
    "Para m = 2: classificação binária\n",
    "Para m > 2: classificação multiclasse\n",
    "\n",
    "- Classificação binária\n",
    "\n",
    "Pergunta:\n",
    "\n",
    "- 1. Dada as características socioeconômicas do indivíduo no survey, é possível classificar de forma assertiva se este indivíduo é depressivo ou não?\n",
    "\n",
    "\n",
    "Tirando esta parte, você precisa avaliar as necessidades de pré-processamento dos dados, como imputar valor faltantes, tratar outliers, normalizar, balancear as classes, etc... Você pode aplicar seleção de atributos se deseja avaliar se um subconjunto dos atributos originais já lhe dar bom poder preditivo.\n",
    "\n",
    "Esse pipeline será integrado a algoritmos de classificação (veja bem, suas saídas são numéricas, mas não se trata de uma regressão. Você tem valores na classe 0 ou 1, pois as classes foram codificadas dessa forma) - e o objetivo será treinar o melhor classificador possível para detectar o risco de depressão.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs de ambiente\n",
    "\n",
    "<hr>\n",
    "SO - Ubuntu 20.4\n",
    "<br>\n",
    "Python version - 3.7.6\n",
    "<br>\n",
    "Conda version version - 4.8.2\n",
    "\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.preprocessing import sequence\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.set_option('display.max_columns', None)\n",
    "import random as r\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## i) análise exploratória dos dados\n",
    "para identificar possíveis problemas nos\n",
    "dados que possam impactar negativamente no treinamento de modelos;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dirname, _, filenames in os.walk('/home/kaline/workspace/mestrado/TrabalhoFinalML/Dataset'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column_name = 'depressed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_depressed_dataset = pd.read_csv(\"Dataset/depressed.csv\")\n",
    "\n",
    "# Create a boxplot of life expectancy per region\n",
    "df_depressed_dataset.boxplot('depressed')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_depressed_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribuição da coluna target (depressed)\n",
    "\n",
    "Classes não balanceadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Contagem de registros por classes\n",
    "from turtle import color\n",
    "\n",
    "\n",
    "target_counts = df_depressed_dataset['depressed'].value_counts()\n",
    "positives = target_counts[1]\n",
    "negatives = target_counts[0]\n",
    "\n",
    "#Proporção em %\n",
    "prop = (positives/negatives)*100\n",
    "print('\\nA variável target do treino possui {}% de positivos.'. format(round(prop,2)))\n",
    "\n",
    "#Gráfico\n",
    "plt.bar(target_counts.index, target_counts,color='red')\n",
    "plt.xticks([0, 1])\n",
    "plt.xlabel('Classe')\n",
    "plt.ylabel('Quantidade')\n",
    "plt.title('Quantidade de registros por classe')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tipos de dados nas colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tipos_dados = df_depressed_dataset.dtypes.value_counts()\n",
    "\n",
    "#Recurso visual\n",
    "plt.bar(tipos_dados.index.astype(str), tipos_dados.values, color='red')\n",
    "plt.ylabel('Quantidade')\n",
    "plt.title('Quantidade de colunas por tipos de dados')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Podemos observar que temos variáveis em diferentes escalas\\nO desvio padrão (std) alto')\n",
    "df_depressed_dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count NAN in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_depressed_dataset.isnull().sum().sort_values().tail(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecção de outliers\n",
    "\n",
    "Melhorar a perfomance do R² (score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(df_depressed_dataset.children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(df_depressed_dataset.edu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(df_depressed_dataset.hh_children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_depressed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_depressed_dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# correlation \n",
    "print(df_depressed_dataset.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ii) pré-processamento dos dados\n",
    "abordando aspectos como correção de\n",
    "outliers e de valores faltantes, codificação de atributos categóricos, discretização de\n",
    "atributos numéricos, normalização, ajuste de desbalanceamento de classes e\n",
    "redução de dimensionalidade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removendo colunas irrelevantes\n",
    "\n",
    "Minha sugestão no primeiro momento é remover apenas as colunas que pela sua definição não parecem ter relevância/poder preditivo, o que eu acredito ser o caso das duas variáveis com \"_id\" no final (não olhei em profundidade os dados). Se as variáveis forem categóricas e tiverem muitos valores possíveis, também pode ser difícil usar no modelo. Neste caso, você pode optar por manter apenas um subconjunto de valores para as categorias mais frequentes, e todas as outras agrupar em uma única categoria (\"Outros\", por exemplo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colunas categoricas\n",
    "print(df_depressed_dataset.select_dtypes(include=['object']).columns.tolist())\n",
    "\n",
    "# Print shape do DataFrame original\n",
    "print(\"Shape do DataFrame original: {}\".format(df_depressed_dataset.shape))\n",
    "\n",
    "# Não tenho tempo para trabalhar com colunas de data agora\n",
    "df_depressed_dataset.drop(columns=['surveyid','village','survey_date','hhsize','cons_nondurable',\n",
    "'asset_livestock','asset_durable','asset_phone','asset_land_owned_total',\n",
    "'asset_niceroof','cons_allfood','cons_ownfood','cons_med_total',\n",
    "'cons_med_children','cons_ed','cons_social','cons_other','ent_wagelabor',\n",
    "'ent_ownfarm','ent_business','ent_nonagbusiness','ent_employees',\n",
    "'ent_nonag_revenue','ent_nonag_flowcost','ent_farmrevenue','ent_farmexpenses',\n",
    " 'ent_animalstockrev','ent_total_cost','fs_adskipm_often','fs_adwholed_often',\n",
    " 'fs_chskipm_often','fs_chwholed_often','fs_meat','fs_enoughtom','fs_sleephun',\n",
    " 'med_expenses_hh_ep','med_expenses_sp_ep','med_expenses_child_ep',\n",
    " 'med_portion_sickinjured','med_port_sick_child','med_afford_port','med_sickdays_hhave',\n",
    " 'med_healthconsult','med_vacc_newborns','med_child_check','med_u5_deaths','ed_expenses',\n",
    " 'ed_expenses_perkid','ed_schoolattend','ed_sch_missedpc',\n",
    " 'ed_work_act_pc','labor_primary','wage_expenditures','early_survey','day_of_week'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Print shape do novo DataFrame\n",
    "print(\"Shape of DataFrame depois as colunas irrelevantes: {}\".format(df_depressed_dataset.shape))\n",
    "# durable_investment,nondurable_investment,given_mpesa,amount_given_mpesa,received_mpesa,amount_received_mpesa,net_mpesa,saved_mpesa,amount_saved_mpesa\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_depressed_dataset.columns\n",
    "df_depressed_dataset.hist(figsize=[20, 15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removendo NAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_depressed_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_depressed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_columns = ['depressed']\n",
    "\n",
    "for df_column in df_columns:\n",
    "   print(df_column)\n",
    "   drop_nan = df_depressed_dataset.drop(df_depressed_dataset[(df_depressed_dataset[df_column].isnull())].index, axis=0, inplace=True)\n",
    "\n",
    "df_filter = df_depressed_dataset.isnull().sum()\n",
    "\n",
    "df_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputing dados faltantes em uma ML Pipeline I\n",
    "# Import o módulo KNNImputer \n",
    "from sklearn.impute import KNNImputer \n",
    "\n",
    "# Setup the Imputation transformer: imputer\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "columnsDropNan = df_depressed_dataset[['hh_totalmembers', 'cons_alcohol', 'cons_tobacco']] \n",
    "df_depressed_dataset[['hh_totalmembers', 'cons_alcohol', 'cons_tobacco']] = imputer.fit_transform(columnsDropNan.values) \n",
    "# Setup the pipeline with the required steps: steps\n",
    "steps = [('imputation', imputer)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filter = df_depressed_dataset.isnull().sum()\n",
    "df_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_depressed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valores ausentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função que deleta registros duplicados da base (Mantendo apenas o primeiro registro a cada grupo de duplicatas)\n",
    "def DupRegClean(df):\n",
    "    # Registros duplicados podem causar ruído nos algoritmos de machine learning portanto iremos excluí-los.\n",
    "    # (Se por acaso colunas duplicadas estierem presentes nos datasets de treino e teste, os resultados podem se mostrar tendenciosos).\n",
    "    print('df antes: ',df.shape)\n",
    "    df_saida = df.drop_duplicates(inplace=False,keep='first')\n",
    "    print('Após a limpeza de registros duplicados: ',df_saida.shape)\n",
    "    return df_saida\n",
    "\n",
    "# Aplica limpeza\n",
    "df1 = DupRegClean(df_depressed_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancemento de classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmath import nan\n",
    "from collections import Counter\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "X = df_depressed_dataset.drop(target_column_name, axis=1).fillna(0)\n",
    "y = df_depressed_dataset[target_column_name]\n",
    "\n",
    "# undersampling e oversampling\n",
    "\n",
    "rus = RandomUnderSampler(random_state=0, sampling_strategy={0:600})\n",
    "x_novo, y_novo = rus.fit_resample(X, y)\n",
    "print(sorted(Counter(y_novo).items()))\n",
    "\n",
    "df_depressed_dataset.iloc[:,:-1] =  x_novo\n",
    "df_depressed_dataset[target_column_name] = y_novo\n",
    "df_depressed_dataset = df_depressed_dataset.dropna()\n",
    "\n",
    "rus = RandomOverSampler(random_state=0, sampling_strategy={1:194})\n",
    "x_novo, y_novo = rus.fit_resample(x_novo, y_novo)\n",
    "print(sorted(Counter(y_novo).items()))\n",
    "\n",
    "df_depressed_dataset.iloc[:,:-1] =  x_novo\n",
    "df_depressed_dataset[target_column_name] = y_novo\n",
    "df_depressed_dataset = df_depressed_dataset.dropna()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_counts = df_depressed_dataset['depressed'].value_counts()\n",
    "positives = target_counts[1]\n",
    "negatives = target_counts[0]\n",
    "\n",
    "#Proporção em %\n",
    "prop = (positives/negatives)*100\n",
    "print('\\nA variável target do treino possui {}% de positivos.'. format(round(prop,2)))\n",
    "\n",
    "#Gráfico\n",
    "plt.bar(target_counts.index, target_counts,color='red')\n",
    "plt.xticks([0, 1])\n",
    "plt.xlabel('Classe')\n",
    "plt.ylabel('Quantidade')\n",
    "plt.title('Quantidade de registros por classe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_depressed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U imbalanced-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_depressed_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_depressed_dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### z-score - remover outliers\n",
    "\n",
    "Z score = (x -mean) / std. deviation\n",
    "\n",
    "- df antes:  (1409, 23)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_columns_z = ['children', 'edu', 'hh_children',\n",
    "       'hh_totalmembers', 'asset_savings', 'cons_alcohol', 'cons_tobacco',\n",
    "       'durable_investment', 'nondurable_investment', 'given_mpesa',\n",
    "       'amount_given_mpesa', 'received_mpesa', 'amount_received_mpesa',\n",
    "       'net_mpesa', 'saved_mpesa', 'amount_saved_mpesa']\n",
    "\n",
    "for column in df_columns_z:\n",
    "    print(column)\n",
    "    m = np.mean(df_depressed_dataset[column])\n",
    "    s = np.std(df_depressed_dataset[column])\n",
    "    print('\\nm\\n', m, '\\ns\\n', s)\n",
    "    df_depressed_dataset['Z-score'] = (df_depressed_dataset[column] - m)/s\n",
    "    df_outlier = df_depressed_dataset[abs(df_depressed_dataset['Z-score']) > 3]\n",
    "    df_depressed_dataset = df_depressed_dataset.drop(df_outlier.index)\n",
    "    print('2 - \\nm\\n', m, '\\ns\\n', s)\n",
    "    sns.boxplot(x=df_depressed_dataset.hh_children)\n",
    "\n",
    "df_depressed_dataset.drop('Z-score', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(df_depressed_dataset.children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_depressed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Setup the pipeline steps: steps\n",
    "steps = [('imputation', imputer)]\n",
    "        \n",
    "# Create the pipeline: pipeline\n",
    "pipeline = Pipeline(steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividindo datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando as variáveis da target\n",
    "X = df_depressed_dataset.drop(target_column_name, axis=1).fillna(0)\n",
    "y = df_depressed_dataset[target_column_name]\n",
    "\n",
    "# Dividindo o dataset em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42,stratify=y)\n",
    "\n",
    "# Observando as proporções de classes nas targets do dataset de treino e teste (o objetivo é ter uma proporção semelhante)\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "treino_b = ax.bar(y_train.value_counts().index-0.2,y_train.value_counts().values,0.4, color='gray',label='Dataset Treino')\n",
    "teste_b  = ax.bar( y_test.value_counts().index+0.2,y_test.value_counts().values ,0.4, color='red' , label='Dataset Teste')\n",
    "\n",
    "# Percentuais\n",
    "perc_tr = (y_train.value_counts().values[1]/y_train.value_counts().values[0])*100\n",
    "print('\\nO dataset de treino possui: ',round(perc_tr,2),'% de classes positivas')\n",
    "perc_ts = (y_test.value_counts().values[1]/y_test.value_counts().values[0])*100\n",
    "print('\\nO dataset de teste possui: ',round(perc_ts,2),'% de classes positivas')\n",
    "\n",
    "# Recurso visual\n",
    "ax.set_ylabel('Quantidade')\n",
    "ax.set_xlabel('Target')\n",
    "ax.set_title('Proporção de classes na variavel target')\n",
    "ax.set_xticks([0,1])\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padronização das features para aplicação do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "column_names_to_normalize = ['children', 'edu', 'hh_children',\n",
    "       'hh_totalmembers', 'asset_savings', 'cons_alcohol', 'cons_tobacco',\n",
    "       'durable_investment', 'nondurable_investment', 'given_mpesa',\n",
    "       'amount_given_mpesa', 'received_mpesa', 'amount_received_mpesa',\n",
    "       'net_mpesa', 'saved_mpesa', 'amount_saved_mpesa']\n",
    "x = df_depressed_dataset[column_names_to_normalize].values\n",
    "x_scaled =  scaler.fit_transform(x)\n",
    "df_temp = pd.DataFrame(x_scaled, columns=column_names_to_normalize, index = df_depressed_dataset.index)\n",
    "df_depressed_dataset[column_names_to_normalize] = df_temp\n",
    "\n",
    "\n",
    "\"\"\" # Padronização das features para aplicação do modelo\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Transformação\n",
    "X_train_ = X_train.copy()\n",
    "print(\"X_train_\", X_train_)\n",
    "X_test_ = X_train.copy()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "print(\"X_train \", X_train)\n",
    "\n",
    "X_test  = scaler.transform(X_test)\n",
    "\n",
    "# Variancia total do dataframe após a padronização\n",
    "std_apos = X_train.std()\n",
    "print('Desvio padrão após a padronização: ', round(std_apos,2))\n",
    "# Transformando os datasets em pandas dataframes para maior facilidade de manipulação\n",
    "X_train = pd.DataFrame(X_train)\n",
    "X_test  = pd.DataFrame(X_test) \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limpeza de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iii) treinamento e validação dos modelos,\n",
    "utilizando as melhores práticas em relação a estratégias de divisão de dados para otimização de hiperparâmetros e\n",
    "seleção de modelos\n",
    "\n",
    " - Acurácia\n",
    " - Precision\n",
    " - Recall\n",
    " - F1 Score\n",
    " - ROC AUC score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# Instancia-se o modelo burro\n",
    "dc = DummyClassifier(strategy=\"most_frequent\") # O Dummy classifier vai prever sempre a target mais frequente no dataset de treino\n",
    "\n",
    "# Treino do modelo burro\n",
    "dc.fit(X_train, y_train)\n",
    "\n",
    "# Teste de performance\n",
    "pred_prob = dc.predict_proba(X_train)\n",
    "preds = dc.predict(X_train)\n",
    "dumb_score = roc_auc_score(y_train, pred_prob[:,1])\n",
    "duymb_acc = accuracy_score(y_train, preds)\n",
    "\n",
    "\n",
    "print('\\nAUC Score modelo dumb: ',dumb_score)\n",
    "print('\\nAccuracy Score modelo dumb: ',round(duymb_acc,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classificação \n",
    "Métodos baseados em instâncias\n",
    "○ K-Vizinhos Mais Próximos\n",
    "● Métodos baseados em procura\n",
    "○ Árvores de decisão\n",
    "● Métodos probabilísticos\n",
    "○ Naïve Bayes\n",
    "● Métodos estatísticos\n",
    "○ Regressão Linear e Regressão Logística\n",
    "● Métodos baseados em otimização\n",
    "○ Redes Neurais Artificiais\n",
    "● Métodos baseados em múltiplos modelos preditivos\n",
    "○ Florestas Aleatórias, Adaboost, e outros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "#from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import recall_score, precision_score, mean_squared_error, confusion_matrix\n",
    "\n",
    "# Parâmetros iniciais para cada algoritmo (Vamos buscar ajustar inicialmente os \n",
    "# hiperparametros para trabalharem com a target desbalanceada)\n",
    "naive_bayes_params = [{}]\n",
    "#xgb_params         = [{\"scale_pos_weight\":5,\"random_state\":42}] # 100% de targets dividido pela quantidade \n",
    "                                                                # de positivos (20%) = 5\n",
    "kneighbors_params  = [{\"weights\":\"distance\"}] \n",
    "svc_params         = [{\"class_weight\":\"balanced\", \"random_state\":42}]\n",
    "log_reg_params     = [{\"class_weight\":\"balanced\",\"max_iter\":10000, \"random_state\":42}]\n",
    "dec_tree_params    = [{\"class_weight\":\"balanced\", \"random_state\":42}]\n",
    "rand_for_params    = [{\"class_weight\":\"balanced\",\"random_state\":42}]\n",
    "mlp_for_params     = [{\"activation\":\"identity\", \"random_state\":42}]\n",
    "\n",
    "modelclasses = [\n",
    "     [\"naive bayes\",       GaussianNB,                 naive_bayes_params]\n",
    "    #,[\"XGBoost\",           XGBClassifier,                      xgb_params]\n",
    "    ,[\"k neighbors\",       KNeighborsClassifier,        kneighbors_params]\n",
    "    ,[\"support vector classifier\", LinearSVC,                  svc_params]\n",
    "    ,[\"log regression\",    LogisticRegression,             log_reg_params]\n",
    "    ,[\"decision tree\",     DecisionTreeClassifier,        dec_tree_params]\n",
    "    ,[\"random forest\",     RandomForestClassifier,        rand_for_params]\n",
    "    ,[\"MLP\",     MLPClassifier,        mlp_for_params]\n",
    "\n",
    "]\n",
    "\n",
    "insights = []\n",
    "#  Treina e testa cada algoritmo com seus respectivos hiperparâmetros\n",
    "for modelname, Model, params_list in modelclasses:\n",
    "    for params in params_list:\n",
    "        model = Model(**params)\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, scoring='roc_auc', cv=5)\n",
    "        score = round(cv_scores.mean(),2)\n",
    "        recall = cross_val_score(model, X_train, y_train, scoring='recall', cv=5)\n",
    "        precision = cross_val_score(model, X_train, y_train, scoring='precision', cv=5)\n",
    "        meanSquaredError = cross_val_score(model, X_train, y_train, scoring='neg_mean_squared_error', cv=5)\n",
    "        recall_mean = recall.mean()\n",
    "        precision_mean = precision.mean()\n",
    "        squared_mean = meanSquaredError.mean()\n",
    "        score_stddev = round(cv_scores.std(),2)\n",
    "        y_pred = cross_val_predict(model, X_train, y_train, cv=10)\n",
    "        conf_mat = confusion_matrix(y_train, y_pred)\n",
    "        insights.append((modelname, model, params, score, score_stddev, recall_mean, precision_mean, conf_mat))\n",
    "\n",
    "        \n",
    "insights.sort(key=lambda x:x[-2], reverse=True)\n",
    "\n",
    "# resultados\n",
    "modelnames = []\n",
    "scores = []\n",
    "for modelname, model, params, score, score_stddev, recall_mean, precision_mean, conf_mat in insights:\n",
    "    print(modelname)\n",
    "    print('\\nScore: ',score, ' Desv_padr: ',score_stddev)\n",
    "    print(\"\\nRecall:\", recall_mean)\n",
    "    print(\"\\nPrecision: \", precision_mean)\n",
    "    print(\"\\nMean squared error: \", squared_mean*(-1))\n",
    "    print(conf_mat)\n",
    "    modelnames.append(modelname)\n",
    "    scores.append(score)\n",
    "    \n",
    "# Recurso visual\n",
    "plt.figure(figsize=(16,5))\n",
    "plt.bar(modelnames, scores, color='red', yerr=score_stddev)\n",
    "plt.ylabel('Lucro')\n",
    "plt.xlabel('Algoritmo')\n",
    "plt.title('Lucro total por algoritmo treinado')\n",
    "plt.axis([-1,len(modelnames),-1,1.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas = X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.I Aprimoramento de hiperparâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve, RandomizedSearchCV\n",
    "\n",
    "class HyperParameterEvaluator:\n",
    "    \"\"\"\n",
    "    Responsible for performing a search in order to discover the set of \n",
    "    specific model configuration arguments that result in the best performance \n",
    "    of the model on a test coverage dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    #           Constructor\n",
    "    # -------------------------------------------------------------------------\n",
    "    def __init__(self, dataframe, regressor, predictable_column, hyperparameters):\n",
    "        \"\"\"\n",
    "        Performs a search in order to discover the set of specific model \n",
    "        configuration arguments that result in the best performance of the model\n",
    "        on a test coverage dataset.\n",
    "        \n",
    "        :param      dataframe: Dataset to be evaluated\n",
    "        :param      regressor: Machine learning algorithm\n",
    "        :param      predictable_column: Column name to be predicted\n",
    "        :param      hyperparameters: Hyperparameters to be analyzed. It must be\n",
    "        a dictionary, where each key is the hyperparameter name and each value\n",
    "        is a list of hyperparameter values to be analyzed\n",
    "        \"\"\"\n",
    "        self.__dataset = dataframe\n",
    "        self.__regressor = regressor\n",
    "        self.__train_score = 0\n",
    "        self.__test_score = 0\n",
    "        self.__predictable_column = predictable_column\n",
    "        self.__hyperparameters = hyperparameters\n",
    "        \n",
    "        \n",
    "    # -------------------------------------------------------------------------\n",
    "    #           Methods\n",
    "    # -------------------------------------------------------------------------\n",
    "    def evaluate_validation_curve(self, metrics):\n",
    "        for name, values in self.__hyperparameters.items():\n",
    "            self.__evaluate_validation_curve(name, values, metrics)\n",
    "        \n",
    "    def __evaluate_validation_curve(self, param_name, param_range, metrics):\n",
    "        self.__build_validation_curve(param_name, param_range, metrics)\n",
    "        self.__build_validation_curve_chart(param_range, param_name)\n",
    "        self.__display_current_chart()\n",
    "        \n",
    "    def __build_validation_curve(self, param_name, param_range, metrics):\n",
    "        self.__train_score, self.__test_score = validation_curve(\n",
    "                self.__regressor,\n",
    "                X = self.__dataset[metrics].values, \n",
    "                y = self.__dataset[self.__predictable_column].values, \n",
    "                param_name = param_name, \n",
    "                param_range = param_range,\n",
    "        )\n",
    "        \n",
    "    def __build_validation_curve_chart(self, param_range, param_name):\n",
    "        self.__build_chart_title(param_name)\n",
    "        self.__build_chart_axis()\n",
    "        self.__build_chart_legend()\n",
    "        self.__build_chart_training_score_data(param_range)\n",
    "        self.__build_chart_cross_validation_score_data(param_range)\n",
    "        \n",
    "    def __build_chart_title(self, param_name):\n",
    "        plt.title(\"Validation Curve - \" + param_name)\n",
    "        \n",
    "    def __build_chart_axis(self):\n",
    "        plt.xlabel(r\"$\\gamma$\")\n",
    "        plt.ylabel(\"Score\")\n",
    "        plt.ylim(0.0, 1.1)\n",
    "        \n",
    "    def __build_chart_legend(self):\n",
    "        plt.legend(loc=\"best\")\n",
    "        \n",
    "    def __build_chart_training_score_data(self, param_range):\n",
    "        train_scores_mean = np.mean(self.__train_score, axis=1)\n",
    "        train_scores_std = np.std(self.__train_score, axis=1)\n",
    "        test_scores_mean = np.mean(self.__test_score, axis=1)\n",
    "        test_scores_std = np.std(self.__test_score, axis=1)\n",
    "        \n",
    "        plt.semilogx(param_range, train_scores_mean, label=\"Training score\",\n",
    "                     color=\"darkorange\", lw=2)\n",
    "        plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                         train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                         color=\"darkorange\", lw=2)\n",
    "        \n",
    "    def __build_chart_cross_validation_score_data(self, param_range):\n",
    "        train_scores_mean = np.mean(self.__train_score, axis=1)\n",
    "        train_scores_std = np.std(self.__train_score, axis=1)\n",
    "        test_scores_mean = np.mean(self.__test_score, axis=1)\n",
    "        test_scores_std = np.std(self.__test_score, axis=1)\n",
    "        \n",
    "        plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\",\n",
    "                     color=\"navy\", lw=2)\n",
    "        plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                         test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                         color=\"navy\", lw=2)\n",
    "        \n",
    "    def __display_current_chart(self):\n",
    "        plt.show()\n",
    "        \n",
    "    def evaluate_hyperparam(self, metrics, hyperparam_space):\n",
    "        hyperparam_table = self.__build_hyperparam_table(metrics, hyperparam_space)\n",
    "        self.__display_hyperparam_table(hyperparam_table)\n",
    "        \n",
    "    def __build_hyperparam_table(self, metrics, hyperparam_space):\n",
    "        hyperparam_table = RandomizedSearchCV(\n",
    "            estimator=self.__regressor, \n",
    "            param_distributions=hyperparam_space,\n",
    "            n_iter=100,\n",
    "            random_state=0\n",
    "        )\n",
    "\n",
    "        hyperparam_table.fit(self.__dataset[metrics].values, self.__dataset[self.__predictable_column].values)\n",
    "        \n",
    "        return hyperparam_table\n",
    "        \n",
    "    def __display_hyperparam_table(self, hyperparam_table):\n",
    "        display(hyperparam_table.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_hyperparams = {\n",
    "    \"activation\": [\"identity\", \"logistic\", \"tanh\", \"relu\"],\n",
    "    \"solver\": [\"lbfgs\", \"sgd\", \"adam\"],\n",
    "    \"alpha\": [x for x in np.linspace(1e-4, 1e4, 10, dtype=float)],\n",
    "    \"tol\": [x for x in np.linspace(1e-4, 1e4, 10, dtype=float)],\n",
    "}\n",
    "metrics = df_depressed_dataset.columns.values[:-1]\n",
    "hpe = HyperParameterEvaluator(\n",
    "    dataframe = df_depressed_dataset,\n",
    "    regressor = MLPClassifier(random_state=0), \n",
    "    predictable_column = \"depressed\", \n",
    "    hyperparameters = mlp_hyperparams\n",
    ")\n",
    "\n",
    "hpe.evaluate_validation_curve(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iv) interpretação do modelo treinado, \n",
    "buscando obter insights sobre o impacto dos atributos na tomada de decisão"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "50c323a00a09e1000073e59d4a0553b298da1d994f91912b7d1d7971920e8959"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
